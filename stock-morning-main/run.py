#!/usr/bin/env python
"""
í†µí•© ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸: SEC í¬ë¡¤ë§ + 4ëª… ì „ë¬¸ê°€ í† ë¡  íŒŒì´í”„ë¼ì¸

ì‚¬ìš©ë²•:
    python run.py --ticker GOOG                    # í¬ë¡¤ë§ + ë¶„ì„
    python run.py --ticker GOOG --skip-crawl       # í¬ë¡¤ë§ ìƒëµ, ë¶„ì„ë§Œ
    python run.py --ticker GOOG --crawl-only       # í¬ë¡¤ë§ë§Œ
    python run.py --ticker GOOG --save             # ê²°ê³¼ JSON ì €ì¥
"""

import argparse
import json
import os
from pathlib import Path
from datetime import datetime
from typing import Optional

from dotenv import load_dotenv

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()


def parse_args():
    parser = argparse.ArgumentParser(
        description="SEC í¬ë¡¤ë§ + 4ëª… ì „ë¬¸ê°€ í† ë¡  íŒŒì´í”„ë¼ì¸"
    )
    parser.add_argument("--ticker", required=True, help="ë¶„ì„í•  í‹°ì»¤ (ì˜ˆ: GOOG, AAPL)")
    parser.add_argument(
        "--skip-crawl",
        action="store_true",
        help="SEC í¬ë¡¤ë§ ìƒëµ (ì´ë¯¸ ë°ì´í„°ê°€ ìˆëŠ” ê²½ìš°)",
    )
    parser.add_argument(
        "--crawl-only",
        action="store_true",
        help="SEC í¬ë¡¤ë§ë§Œ ì‹¤í–‰ (ë¶„ì„ ìƒëµ)",
    )
    parser.add_argument(
        "--no-save",
        action="store_true",
        help="ê²°ê³¼ JSON íŒŒì¼ ì €ì¥ ì•ˆ í•¨ (ê¸°ë³¸: ì €ì¥)",
    )
    parser.add_argument(
        "--output-dir",
        type=str,
        default="data/agent_results",
        help="ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬ (ê¸°ë³¸: data/agent_results)",
    )
    return parser.parse_args()


def run_crawling(ticker: str) -> dict:
    """SEC í¬ë¡¤ë§ ì‹¤í–‰"""
    from src.sec_crawler import SECCrawler
    from src.db import SECDatabase
    
    print("\n" + "=" * 100)
    print("ğŸ“¥ SEC í¬ë¡¤ë§ ì‹œì‘")
    print("=" * 100)
    
    sec_crawler = SECCrawler()
    db = SECDatabase()
    
    print(f"\n[{ticker}] SEC ê³µì‹œ í¬ë¡¤ë§ ì¤‘...")
    results = sec_crawler.crawl_filings_in_window(
        ticker,
        save_to_db=True,
        db=db,
        only_today=True,
        include_annual_quarterly=True,  # 10-K, 10-Q í•­ìƒ í¬í•¨
    )
    
    stats = {"total": 0, "10-K": False, "10-Q": False}
    if results:
        for metadata, file_path in results:
            form = metadata.get('form')
            print(f"  âœ… {form}: {file_path}")
            stats["total"] += 1
            if form == "10-K":
                stats["10-K"] = True
            if form == "10-Q":
                stats["10-Q"] = True
    else:
        print(f"  âšª ìƒˆë¡œìš´ ê³µì‹œ ì—†ìŒ (ê¸°ì¡´ ë°ì´í„° ì‚¬ìš©)")
    
    print(f"\nğŸ“Š í¬ë¡¤ë§ ê²°ê³¼: {stats['total']}ê±´ (10-K: {'âœ…' if stats['10-K'] else 'âŒ'}, 10-Q: {'âœ…' if stats['10-Q'] else 'âŒ'})")
    print("=" * 100)
    
    return stats


def run_analysis(ticker: str, save: bool = False, output_dir: str = "data/agent_results") -> dict:
    """4ëª… ì „ë¬¸ê°€ í† ë¡  íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
    from multiagent.graph import run_multiagent_pipeline
    
    # LangSmith ì¶”ì  ìƒíƒœ í™•ì¸
    langsmith_enabled = os.getenv("LANGCHAIN_TRACING_V2") == "true"
    langsmith_project = os.getenv("LANGCHAIN_PROJECT", "stock-morning")
    
    print("\n" + "=" * 100)
    print(f"ğŸ¯ 4-EXPERT DEBATE PIPELINE START")
    print(f"ğŸ“Š Ticker: {ticker}")
    print(f"â° Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    if langsmith_enabled:
        print(f"ğŸ” LangSmith Tracing: âœ… Enabled (Project: {langsmith_project})")
        print(f"   ğŸ“ https://smith.langchain.com/o/{os.getenv('LANGSMITH_ORG', 'default')}/projects/p/{langsmith_project}")
    else:
        print(f"ğŸ” LangSmith Tracing: âš ï¸  Disabled")
    print("=" * 100)
    
    # íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
    result = run_multiagent_pipeline(ticker)
    
    # JSON ì €ì¥
    if save:
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"{ticker}_{timestamp}_debate.json"
        filepath = output_path / filename
        
        structured_conclusion = result.get("structured_conclusion")
        
        save_data = {
            "ticker": ticker,
            "timestamp": timestamp,
            "rounds": result.get("rounds", []),
            "moderator_analyses": result.get("moderator_analyses", []),  # ì¤‘ì¬ì ë¶„ì„ (í•©ì˜ì , ìŸì , ê°€ì´ë“œ)
            "conclusion": result.get("conclusion", ""),
            "readable_summary": result.get("readable_summary", ""),
            "debate_transcript": result.get("debate_transcript", ""),
            "sources": result.get("sources", {}),  # ê²€ì¦ ì—ì´ì „íŠ¸ìš© ì¶œì²˜ ì •ë³´
        }
        
        if structured_conclusion:
            save_data["structured_conclusion"] = structured_conclusion.model_dump()
        
        with open(filepath, "w", encoding="utf-8") as f:
            json.dump(save_data, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ’¾ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {filepath}")
    
    return result


def main():
    args = parse_args()
    ticker = args.ticker.upper()
    
    print("\n" + "=" * 100)
    print(f"ğŸš€ STOCK MORNING - í†µí•© ë¶„ì„ íŒŒì´í”„ë¼ì¸")
    print(f"ğŸ“Š Ticker: {ticker}")
    print(f"â° ì‹œì‘ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 100)
    
    # 1ë‹¨ê³„: SEC í¬ë¡¤ë§
    if not args.skip_crawl:
        crawl_stats = run_crawling(ticker)
    else:
        print("\nâ­ï¸  SEC í¬ë¡¤ë§ ìƒëµ (--skip-crawl)")
    
    # 2ë‹¨ê³„: ì „ë¬¸ê°€ í† ë¡  ë¶„ì„
    if not args.crawl_only:
        save = not args.no_save  # ê¸°ë³¸: ì €ì¥, --no-save ì‹œ ì €ì¥ ì•ˆ í•¨
        result = run_analysis(ticker, save=save, output_dir=args.output_dir)
        
        # 3ë‹¨ê³„: ì‚¬ìš©í•˜ì§€ ì•Šì€ íŒŒì¼ë§Œ ì‚­ì œ (ê²€ì¦ìš© ë°ì´í„° ìœ ì§€)
        cleanup_unused_files(ticker, result)
    else:
        print("\nâ­ï¸  ë¶„ì„ ìƒëµ (--crawl-only)")
        result = None
    
    # ì™„ë£Œ
    print("\n" + "=" * 100)
    print("âœ¨ PIPELINE COMPLETED")
    print(f"â° ì¢…ë£Œ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 100)
    
    return result


def cleanup_unused_files(ticker: str, result: dict):
    """ì„ì‹œ íŒŒì¼ ì •ë¦¬ (ë‰´ìŠ¤ ì „ì²´ ì‚­ì œ - pkë¡œ DynamoDB ì¬ì¡°íšŒ ê°€ëŠ¥)"""
    import shutil
    
    sources = result.get("sources", {})
    
    # 1. ë‰´ìŠ¤ íŒŒì¼ ì „ì²´ ì‚­ì œ (ê²€ì¦ ì—ì´ì „íŠ¸ëŠ” pkë¡œ DynamoDB ì§ì ‘ ì¡°íšŒ)
    aws_results_dir = Path("aws_results")
    if aws_results_dir.exists():
        ticker_files = list(aws_results_dir.glob(f"{ticker}_*.json"))
        for f in ticker_files:
            f.unlink()
        if ticker_files:
            print(f"\nğŸ§¹ ë‰´ìŠ¤ ì„ì‹œ íŒŒì¼ ì‚­ì œ: {len(ticker_files)}ê°œ")
    
    # 2. SEC íŒŒì¼ ì •ë¦¬: sourcesì— ìˆëŠ” íŒŒì¼ + 10-K/10-QëŠ” í•­ìƒ ìœ ì§€
    sec_dir = Path("downloads/sec_filings")
    if sec_dir.exists():
        # ìƒˆ sources êµ¬ì¡°: sources["sources"] ë°°ì—´ì—ì„œ type="sec_filing" ì¶”ì¶œ
        all_sources = sources.get("sources", [])
        used_accessions = set()
        for item in all_sources:
            if item.get("type") == "sec_filing":
                acc = item.get("accession_number", "")
                if acc:
                    # 0001652044-25-000014 -> 000165204425000014
                    used_accessions.add(acc.replace("-", ""))
        
        kept_count = 0
        deleted_count = 0
        
        for f in sec_dir.glob(f"*{ticker}*") if ticker else sec_dir.glob("*.xml"):
            stem = f.stem
            
            # 10-K/10-QëŠ” í•­ìƒ ìœ ì§€ (FilingSummary.xml í¬í•¨)
            if "FilingSummary" in stem:
                kept_count += 1
                continue
            
            # sourcesì— ìˆëŠ” íŒŒì¼ë§Œ ìœ ì§€
            is_used = any(acc in stem for acc in used_accessions)
            
            if is_used:
                kept_count += 1
            else:
                f.unlink()
                deleted_count += 1
        
        if kept_count > 0 or deleted_count > 0:
            print(f"ğŸ§¹ SEC íŒŒì¼ ì •ë¦¬: {kept_count}ê°œ ìœ ì§€, {deleted_count}ê°œ ì‚­ì œ")


if __name__ == "__main__":
    main()
